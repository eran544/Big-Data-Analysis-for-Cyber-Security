{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this task you will play around with character level n-grams, one-hot encoding, and tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:02:28.39861Z",
     "start_time": "2019-02-06T08:02:28.388108Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = {0: \"This Is a Simple sentence\", \n",
    "          1: \"this is another simple one, all lower case..\", \n",
    "          2: \"Hi! How are you? Please update to this version 1.1.1\",\n",
    "          3: \"please update to this version 1.1.2 hi! how are you?\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:02:40.09826Z",
     "start_time": "2019-02-06T08:02:40.092699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lower/upper case?\n",
    "allLowerCase=True\n",
    "# Do you use words or charcters n-grams?\n",
    "# What delimiters would you use?\n",
    "delimiters = r'[!? ]' \n",
    "# How do you treat digits?\n",
    "convertDigitsToAsterisk=True\n",
    "# How long is the n-gram?\n",
    "n=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:02:40.598129Z",
     "start_time": "2019-02-06T08:02:40.5799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of ngrams:\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ngram2df = dict()\n",
    "ngram2tf = dict()\n",
    "ngram2index = dict()\n",
    "for sentence_id in corpus.keys(): \n",
    "    sentence=corpus[sentence_id]\n",
    "    if convertDigitsToAsterisk:\n",
    "        sentence = re.sub(r'[0-9]', '*', sentence)\n",
    "    if allLowerCase:\n",
    "        sentence = sentence.lower()\n",
    "    for word in re.split(delimiters,sentence):\n",
    "        # Thanks to Shaked and Royee for fixing this bug :-)\n",
    "        i=n\n",
    "        while i<=len(word):\n",
    "            if i>=n:\n",
    "                token = word[i-n:i]            \n",
    "                if not(token in ngram2df.keys()):\n",
    "                    ngram2tf[token] = [0]*len(corpus.keys())\n",
    "                    ngram2df[token]=set()\n",
    "                    ngram2index[token] = len(ngram2index.keys())\n",
    "                ngram2tf[token][sentence_id]+=1\n",
    "                ngram2df[token].add(sentence_id)\n",
    "            i+=1\n",
    "print('Num of ngrams:')\n",
    "print(len(ngram2tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: How different values affect the amount of ngrams? Play with n and allLowerCase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect ngram2tf and ngram2df - make sure they make sense.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:02:40.598129Z",
     "start_time": "2019-02-06T08:02:40.5799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': [1, 1, 1, 1],\n",
       " 'simp': [1, 1, 0, 0],\n",
       " 'impl': [1, 1, 0, 0],\n",
       " 'mple': [1, 1, 0, 0],\n",
       " 'sent': [1, 0, 0, 0],\n",
       " 'ente': [1, 0, 0, 0],\n",
       " 'nten': [1, 0, 0, 0],\n",
       " 'tenc': [1, 0, 0, 0],\n",
       " 'ence': [1, 0, 0, 0],\n",
       " 'anot': [0, 1, 0, 0],\n",
       " 'noth': [0, 1, 0, 0],\n",
       " 'othe': [0, 1, 0, 0],\n",
       " 'ther': [0, 1, 0, 0],\n",
       " 'one,': [0, 1, 0, 0],\n",
       " 'lowe': [0, 1, 0, 0],\n",
       " 'ower': [0, 1, 0, 0],\n",
       " 'case': [0, 1, 0, 0],\n",
       " 'ase.': [0, 1, 0, 0],\n",
       " 'se..': [0, 1, 0, 0],\n",
       " 'plea': [0, 0, 1, 1],\n",
       " 'leas': [0, 0, 1, 1],\n",
       " 'ease': [0, 0, 1, 1],\n",
       " 'upda': [0, 0, 1, 1],\n",
       " 'pdat': [0, 0, 1, 1],\n",
       " 'date': [0, 0, 1, 1],\n",
       " 'vers': [0, 0, 1, 1],\n",
       " 'ersi': [0, 0, 1, 1],\n",
       " 'rsio': [0, 0, 1, 1],\n",
       " 'sion': [0, 0, 1, 1],\n",
       " '*.*.': [0, 0, 1, 1],\n",
       " '.*.*': [0, 0, 1, 1]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngram2df\n",
    "ngram2tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:03:46.749869Z",
     "start_time": "2019-02-06T08:03:46.72921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [4, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1: [4, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 2: [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 3: [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "sentence2ngrams = dict()\n",
    "for sentence_id in corpus.keys():\n",
    "    sentence=corpus[sentence_id]\n",
    "    if convertDigitsToAsterisk:\n",
    "        sentence = re.sub(r'[0-9]', '*', sentence)\n",
    "    if allLowerCase:\n",
    "        sentence = sentence.lower()\n",
    "    sentence2ngrams[sentence_id] = [0]*len(ngram2index)\n",
    "    for word in re.split(delimiters,sentence):\n",
    "        i=len(word)-n\n",
    "        while i<=len(word):\n",
    "            if i>=n:\n",
    "                token = word[i-n:i]            \n",
    "                sentence2ngrams[sentence_id][ngram2index[token]] = len(ngram2df[token])\n",
    "            i+=1\n",
    "print(sentence2ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect sentence2ngrams - make sure they make sense.. \n",
    "## Q: what does a high number represent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set the params like below and rerun the above.\n",
    "allLowerCase=True\n",
    "delimiters = r'[!? ]' \n",
    "convertDigitsToAsterisk=True\n",
    "n=5\n",
    "\n",
    "## Q: Note that the last two entires in sentence2ngrams are the same. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:09:56.751868Z",
     "start_time": "2019-02-06T08:09:56.730801Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "def tfidf(tf,df,corpus_size):\n",
    "    idf = math.log(float(corpus_size)/float(df),10)\n",
    "    if idf == 0:\n",
    "        return 0\n",
    "    return (float(tf)/idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T08:09:56.751868Z",
     "start_time": "2019-02-06T08:09:56.730801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 3.321928094887363, 3.321928094887363, 3.321928094887363, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1: [0, 3.321928094887363, 3.321928094887363, 3.321928094887363, 0, 0, 0, 0, 0, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 1.6609640474436815, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 2: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363], 3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363, 3.321928094887363]}\n"
     ]
    }
   ],
   "source": [
    "sentence2tfidf = dict()\n",
    "for sentence_id in corpus.keys():\n",
    "    sentence=corpus[sentence_id]\n",
    "    if convertDigitsToAsterisk:\n",
    "        sentence = re.sub(r'[0-9]', '*', sentence)\n",
    "    if allLowerCase:\n",
    "        sentence = sentence.lower()\n",
    "    sentence2tfidf[sentence_id] = [0]*len(ngram2index)\n",
    "    for word in re.split(delimiters,sentence):\n",
    "        i=len(word)-n\n",
    "        while i<=len(word):\n",
    "            if i>=n:\n",
    "                token = word[i-n:i]     \n",
    "                tf = ngram2tf[token][sentence_id]\n",
    "                df = len(ngram2df[token])\n",
    "                sentence2tfidf[sentence_id][ngram2index[token]] = tfidf(tf,df,len(corpus.keys()))\n",
    "            i+=1\n",
    "print(sentence2tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: inspect the value assigned to the the tokens \"this\" and  \"another\". (to do that you will first need to find their indexes. What are their values? Why are they different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
